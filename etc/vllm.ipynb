{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -d --runtime nvidia --gpus all \\\n",
    "    --shm-size=384G \\\n",
    "    -v /workspace01/lecturer/vllm/models/Mistral-7B-Instruct-v0.3:/app/models/Mistral-7B-Instruct-v0.3 \\\n",
    "    --name mistral:latest \\\n",
    "    -p 11430:11430 --ipc=host \\\n",
    "    vllm/vllm-openai:latest \\\n",
    "    --model \\\n",
    "    /app/models/Mistral-7B-Instruct-v0.3 \\\n",
    "        --served-model-name Mistral-7B-Instruct-v0.3 \\\n",
    "        --max-num-seq=41 \\\n",
    "        --tensor-parallel-size=4 \\\n",
    "        --gpu-memory-utilization=0.9 \\\n",
    "        --enforce-eager \\\n",
    "        --chat-template-content-format=openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8339641",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -d --runtime nvidia --gpus all \\\n",
    "    --shm-size=384G \\\n",
    "    -v /data2/models:/app/models \\\n",
    "    --name mistral-small:latest \\\n",
    "    -p 8080:8000 --ipc=host \\\n",
    "    vllm/vllm-openai:latest \\\n",
    "    --model \\\n",
    "    /app/models/Llama-3.1-70B-Instruct \\\n",
    "        --served-model-name LLAMA-3.1-70B \\\n",
    "        --max-num-seq=41 \\\n",
    "        --tensor-parallel-size=4 \\\n",
    "        --gpu-memory-utilization=0.9 \\\n",
    "        --enforce-eager \\\n",
    "        --chat-template-content-format=openai\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
