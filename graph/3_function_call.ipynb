{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99d09e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "# from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from function_call_tool import tools\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d47f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "통일된 interface 통해 function calling test\n",
    "\n",
    "ollama 모델 사용:\n",
    "1. download ollama\n",
    "$ curl -fsSL https://ollama.com/install.sh | sh\n",
    "2. pull model with model name in https://ollama.com/search\n",
    "$ ollama run {model_name}\n",
    "\"\"\"\n",
    "\n",
    "MODEL_LIST = [\n",
    "    # \"gpt-4o-mini\",\n",
    "    # \"gpt-4o\",\n",
    "    # ollama interface\n",
    "    \"qwen2.5:7b\",\n",
    "    # \"qwen2.5:14b\",\n",
    "    # \"llama3.1:8b\",\n",
    "    # huggingface\n",
    "    # \"Jungwonchang/Ko-QWEN-7B-Chat-LoRA\",\n",
    "    # \"etri-xainlp/qwen-ko-14b-instruct\",\n",
    "    # \"Saxo/Linkbricks-Horizon-AI-Korean-llama3.1-sft-rlhf-dpo-8B\",\n",
    "    # \"SEOKDONG/llama3.1_korean_v1.1_sft_by_aidx\"\n",
    "]\n",
    "\n",
    "QUESTION_LIST = [\n",
    "    \"주문 조회해줘\",\n",
    "    \"내 등급 조회\",\n",
    "    \"스트라이프 셔츠 주문한거 언제와\",\n",
    "    \"내 등급이랑 주문정보 알려줘\", # multi tool use\n",
    "    \"한국의 수도가 어디야\"\n",
    "]\n",
    "\n",
    "# llm_invoke_callbacks = [ConsoleCallbackHandler()]\n",
    "llm_invoke_callbacks = []\n",
    "\n",
    "tool_dict = {t.name:t for t in tools}\n",
    "\n",
    "def get_model(model_name):\n",
    "    if \"gpt\" in model_name:\n",
    "        model = ChatOpenAI(model_name=model_name)\n",
    "    elif \":\" in model_name:\n",
    "        model = ChatOllama(model=model_name, base_url = \"http://192.168.1.203:11434\")\n",
    "    else:\n",
    "        model = HuggingFacePipeline.from_model_id(\n",
    "            model_id=model_name,\n",
    "            task=\"text-generation\",\n",
    "            pipeline_kwargs={\"max_new_tokens\":512}\n",
    "        )\n",
    "    llm_with_tools = model.bind_tools(tools)\n",
    "    return llm_with_tools\n",
    "\n",
    "def invoke_model(query, llm):\n",
    "    messages = [HumanMessage(query)]\n",
    "\n",
    "    ai_msg = llm.invoke(messages, config={'callbacks': llm_invoke_callbacks})\n",
    "    messages.append(ai_msg)\n",
    "    # return response\n",
    "    if ai_msg.content=='':\n",
    "        # tool call\n",
    "        for tool_call in ai_msg.tool_calls:\n",
    "            selected_tool = tool_dict[tool_call['name']]\n",
    "            print(f\"* {selected_tool.name}() tool 사용중...\")\n",
    "            tool_msg = selected_tool.invoke(tool_call)\n",
    "            messages.append(tool_msg)\n",
    "        response2 = llm.invoke(messages, config={'callbacks': llm_invoke_callbacks})\n",
    "        answer = response2.content\n",
    "    else:\n",
    "        answer = ai_msg.content\n",
    "    return answer\n",
    "\n",
    "\n",
    "def main(model_name):\n",
    "    print(f\"\\n\\n< MODEL : {model_name} >\\n\")\n",
    "    llm = get_model(model_name)\n",
    "\n",
    "    for q in QUESTION_LIST:\n",
    "        print(f\"Q: {q}\")\n",
    "        t0 = time.time()\n",
    "        answer = invoke_model(q, llm)\n",
    "        print(f\"A : {answer}\\n\")\n",
    "        took = time.time() - t0\n",
    "        print(f\"\\nTook {round(took, 4)}s\")\n",
    "        print(\"-\"*50) \n",
    "\n",
    "\n",
    "\n",
    "for model in MODEL_LIST:\n",
    "    main(model)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
